{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c4a42a8-71b3-437f-b05f-19e99e24d521",
   "metadata": {},
   "source": [
    "# Interact with Deepseek-R1 via Ollama Python\n",
    "\n",
    "In this notebook we will learn how to\n",
    "\n",
    "1. List open models installed on your local machine\n",
    "2. Stream responses from deepseek-r1\n",
    "3. Track conversation history.\n",
    "\n",
    "> Ollama python package can be installed from [PyPI](https://pypi.org/project/ollama/). I have provided a [conda environment](./environment.yml) file that will install it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e204f-0c15-4d70-845d-3ddff4afda2f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437032e-5656-491b-a0cd-9c1c07fb4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495cbcd-7474-4a56-ae4a-fa7fcddaba84",
   "metadata": {},
   "source": [
    "## Open models installed on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c278f1e-9dae-4731-a84e-0571a292dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def installed_models():\n",
    "    '''\n",
    "    Iterate through ollama models and return names as list\n",
    "    '''\n",
    "    return [md.model for md in ollama.list().models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff5930-932a-43b1-ae73-70533fe32104",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_models = installed_models()\n",
    "local_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd7a4e-4786-48b8-9f9f-cb2cf8d12523",
   "metadata": {},
   "source": [
    "## Stream Chat Responses from a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48319e-fe11-4ab1-a2b9-68079d4bd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(model_name: str, prompt_msg: str):\n",
    "    '''get a streaming chat from a model\n",
    "    '''\n",
    "    stream = chat(\n",
    "        model=model_name,\n",
    "        messages=[{'role': 'user', 'content': prompt_msg}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b6c78-f0e8-4b5c-b8a3-56bf931cece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "Given five positive integers in a list, find the minimum and maximum values that\n",
    "can be calculated by summing exactly four of the five integers. \n",
    "Print the respective minimum and maximum values. \n",
    "Code the solution in Python as a python function that accepts a Python list \n",
    "as a parameter.\n",
    "\n",
    "Example\n",
    "\n",
    "input  = [9, 3, 5, 7, 1]\n",
    "Output = 16, 24\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c1142-8200-4509-9c8e-1bb3fba2adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use coder first.\n",
    "stream = get_stream(local_models[0], prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a14552-b4e8-492a-b058-d526dc16860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_fname = 'response_1.txt'\n",
    "with open(response_fname, 'w') as writer:\n",
    "    for chunk in stream:\n",
    "      print(chunk['message']['content'], end='', flush=True)\n",
    "      writer.write(chunk['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d0d12-2da4-46fe-a3d5-a63f462d33e7",
   "metadata": {},
   "source": [
    "## Conversation history\n",
    "\n",
    "> **Acknowledgement**: The format shown below is taken from the `ollama-python` [github](https://github.com/ollama/ollama-python/blob/main/examples/chat-with-history.py)\n",
    "\n",
    "```python\n",
    "history = [\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "  {\n",
    "    'role': 'assistant',\n",
    "    'content': \"The sky is blue because of the way the Earth's atmosphere scatters sunlight.\",\n",
    "  },\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What is the weather in Tokyo?',\n",
    "  },\n",
    "  {\n",
    "    'role': 'assistant',\n",
    "    'content': '...',\n",
    "  },\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655f26a-86c8-454b-a03b-703c54964e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(model_name: str, chat_history: list):\n",
    "    '''get a streaming chat from a model\n",
    "    '''\n",
    "    stream = chat(\n",
    "        model=model_name,\n",
    "        messages=chat_history,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9a875-0597-4b1c-b1b8-1836cc4a63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_message(history: list, role: str, content: str):\n",
    "    '''Helper function for updating the message history\n",
    "    '''\n",
    "    prompt = {\n",
    "        'role': role,\n",
    "        'content': content\n",
    "    }\n",
    "    history.append(prompt)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f683b8c-0923-481d-9e96-527779390bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_response(file_name: str):\n",
    "    '''Read a stored response from file\n",
    "    '''\n",
    "    response = \"\"\n",
    "    with open(file_name, 'r') as reader:\n",
    "        for line in reader:\n",
    "            response += line.rstrip('\\n')\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846749c5-f88f-47ed-b881-b17f269355f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5553b3-8e70-4257-ba7f-acbe1c970509",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = format_message(messages, 'user', prompt_1)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf2851-0291-43f4-8b55-06940469334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_response = read_response(response_fname)\n",
    "messages = format_message(messages, 'assistant', stored_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551d64c-9be2-4a86-9cc9-a615034f53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"split the solution into two functions for the max and min sums respectively.\"\n",
    "messages = format_message(messages, 'user', prompt_2)\n",
    "stream = get_stream(local_models[0], messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a38ed1-af83-4656-92e6-6ddea4a974c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_fname = 'response_2.txt'\n",
    "with open(response_fname, 'w') as writer:\n",
    "    for chunk in stream:\n",
    "      print(chunk['message']['content'], end='', flush=True)\n",
    "      writer.write(chunk['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
