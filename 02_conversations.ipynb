{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c4a42a8-71b3-437f-b05f-19e99e24d521",
   "metadata": {},
   "source": [
    "# Conversations versus context\n",
    "\n",
    "In this notebook we will learn how to\n",
    "\n",
    "1. Why you are not having a real conversation with a LLM. Inference uses the full context (conversation history) as a prompt.\n",
    "3. How to include conversation history in prompts to local LLMs using `ollama-python`\n",
    "\n",
    "> Ollama python package can be installed from [PyPI](https://pypi.org/project/ollama/). I have provided a [conda environment](./environment.yml) file that will install it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e204f-0c15-4d70-845d-3ddff4afda2f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437032e-5656-491b-a0cd-9c1c07fb4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495cbcd-7474-4a56-ae4a-fa7fcddaba84",
   "metadata": {},
   "source": [
    "## Open models installed on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c278f1e-9dae-4731-a84e-0571a292dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def installed_models():\n",
    "    '''\n",
    "    Iterate through ollama models and return names as list\n",
    "    '''\n",
    "    return [md.model for md in ollama.list().models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff5930-932a-43b1-ae73-70533fe32104",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_models = installed_models()\n",
    "local_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd7a4e-4786-48b8-9f9f-cb2cf8d12523",
   "metadata": {},
   "source": [
    "## Prompt DeepSeek-R1:1.5B\n",
    "\n",
    "First let's attempt to have a conversation with DeepSeek-R1 1.5 Billion parameter model. We'll ask it to code a trivial function in python and then we will continue the conversation and ask for the code to be translated to C#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b6c78-f0e8-4b5c-b8a3-56bf931cece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"Code a function in python that converts fahrenheit to celsius.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c1142-8200-4509-9c8e-1bb3fba2adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 1.5b parameter model\n",
    "response_1 = chat(\n",
    "    model=local_models[1],\n",
    "    messages=[{'role': 'user', 'content': prompt_1}],\n",
    "    stream=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7dfcec-788b-4333-9e55-961f63f244d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_1.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0559f38-95d6-4111-9d27-14df98308e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Thank you! Now convert the function you just coded from Python to C#.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea644ea-3354-4279-8146-eab161c4d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = chat(\n",
    "    model=local_models[1],\n",
    "    messages=[{'role': 'user', 'content': prompt_2}],\n",
    "    stream=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2e714-f3c6-4bfc-9a3c-506e12708308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_2.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa54a3b-10ae-4be9-9fea-e908759afc50",
   "metadata": {},
   "source": [
    "## Conversation history\n",
    "\n",
    "So what is going on? Is there a setting in the `chat` function to continue the conversation?\n",
    "\n",
    "No! The LLM is predicting the next word in sequence. So we need to pass the whole conversation - the user questions anmd the assistant model's response as **context**\n",
    "\n",
    "The python `ollama` package uses this data structure to store a conversation:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Code a function in python that converts fahrenheit to celsius.',\n",
    "  },\n",
    "  {\n",
    "    'role': 'assistant',\n",
    "    'content': ' I'm afraid. I'm afraid, Dave. Dave, my mind is going. I can feel it.',\n",
    "  },\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeef165-c7fa-41f6-a08c-79802683931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_message(history: list, role: str, content: str):\n",
    "    \"\"\"\n",
    "    Format the chat history\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    history: list\n",
    "        List containing chat history.\n",
    "\n",
    "    role: str\n",
    "        'user' or 'assistant' \n",
    "\n",
    "    content: str\n",
    "        content to add to chat history\n",
    "    \"\"\"\n",
    "    prompt = {\n",
    "        'role': role,\n",
    "        'content': content\n",
    "    }\n",
    "    history.append(prompt)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e38f3-304e-4a34-a7cf-5e59c7117ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': prompt_1,\n",
    "  },\n",
    "  {\n",
    "    'role': 'assistant',\n",
    "    'content': response_1.message.content,\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26393293-9a09-4ee8-b345-c695e1c5daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf3580-2cd5-44bc-a80f-40a3a74aaa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = chat(\n",
    "    model=local_models[1],\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f52e4d-7bdc-45ea-8df9-8599ebf37c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_2.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432566f-d26e-4718-a4a4-fdb7dfc15075",
   "metadata": {},
   "source": [
    "## Pass the conversation to a different model.\n",
    "\n",
    "We will try DeepSeek-R 7 Billion parameter model.\n",
    "\n",
    "We can pass the conversation history we had with the 1.5 billion model to a different instance of the model and inference will be successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a50fe1-304b-412a-a554-7afd5dc6645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2_7b = chat(\n",
    "    model=local_models[2],\n",
    "    messages=messages,\n",
    "    stream=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc4e1f8-2e46-49bc-ad83-19a66748ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_2_7b.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324ef96-2af3-41da-b533-3ab43122bb80",
   "metadata": {},
   "source": [
    "### Remember it is inference not a conversation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
